{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw-uDLUDl-a8"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Project: Deep Learning Benchmark for PV Power Forecasting (Deep_Robust Regime)\n",
        "# File: benchmark_comparison.ipynb\n",
        "# Description: Benchmarking 9 models (including Transformer) under identical\n",
        "#              hyperparameter conditions to ensure fair comparison.\n",
        "# Environment: Google Colab / TensorFlow 2.x\n",
        "# =============================================================================\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 0. Setup & Configuration\n",
        "# ---------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from google.colab import files\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Dense, Dropout, LSTM, GRU, SimpleRNN,\n",
        "                                     Conv1D, BatchNormalization, Bidirectional,\n",
        "                                     MaxPooling1D, GlobalAveragePooling1D,\n",
        "                                     MultiHeadAttention, LayerNormalization, Add,\n",
        "                                     Concatenate, Flatten, Activation)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Robust Reproducibility\n",
        "SEED = 42\n",
        "def set_seeds(seed=SEED):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    try:\n",
        "        tf.config.experimental.enable_op_determinism()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "set_seeds()\n",
        "\n",
        "# [Configuration] Deep_Robust Settings\n",
        "CONFIG = {\n",
        "    'SEQ_LENGTH': 24,           # Input Sequence Length\n",
        "    'UNITS': 128,               # Model Capacity\n",
        "    'DROPOUT': 0.5,             # Regularization\n",
        "    'KERNEL_SIZE': 3,           # CNN Kernel\n",
        "    'ATTN_HEADS': 4,            # Attention Heads\n",
        "    'FF_DIM': 128,              # Transformer FeedForward Dim\n",
        "    'ACTIVATION': 'swish',      # Activation Function\n",
        "    'LEARNING_RATE': 0.0005,    # Optimizer LR\n",
        "    'BATCH_SIZE': 32,           # Batch Size\n",
        "    'EPOCHS': 30,               # Max Epochs\n",
        "    'OPTIMIZER': 'adam',        # Optimizer\n",
        "    'LOSS': 'mse'               # Loss Function\n",
        "}\n",
        "\n",
        "print(\"--- [Configuration] Deep_Robust Settings Applied ---\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\" > {k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 1. Data Loading & Preprocessing\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n--- [Step 1] Data Preparation ---\")\n",
        "print(\"Please upload your dataset (e.g., 'Dangjin_Landfill_PV_Dataset.csv' or 'Gwangyang_Port_Phase2_PV.csv').\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))\n",
        "df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "\n",
        "def add_cyclical_features(df):\n",
        "    \"\"\"Adds cyclical time encoding for seasonality capture.\"\"\"\n",
        "    df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "    df['DaysInYear'] = df['Date'].dt.is_leap_year.apply(lambda x: 366 if x else 365)\n",
        "\n",
        "    # Yearly Seasonality\n",
        "    df['Day_sin'] = np.sin(2 * np.pi * df['DayOfYear'] / df['DaysInYear'])\n",
        "    df['Day_cos'] = np.cos(2 * np.pi * df['DayOfYear'] / df['DaysInYear'])\n",
        "\n",
        "    # Daily Seasonality\n",
        "    df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24.0)\n",
        "    df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24.0)\n",
        "    return df\n",
        "\n",
        "df = add_cyclical_features(df)\n",
        "\n",
        "# Feature Definition\n",
        "weather_cols = ['AverageTemp', 'LowTemp', 'HighTemp', 'RainFall', 'SteamPress',\n",
        "                'DewPoint', 'Sunshine', 'Insolation', 'Cloudiness', 'GroundTemp',\n",
        "                'Temp', 'Wind', 'Press', 'Humi']\n",
        "time_cols = ['Day_sin', 'Day_cos', 'Hour_sin', 'Hour_cos']\n",
        "feature_cols = weather_cols + time_cols\n",
        "target_col = 'Solar_Power'\n",
        "\n",
        "# Chronological Split\n",
        "train_df = df[df['Year'].isin([2015, 2016, 2017])]\n",
        "val_df = df[df['Year'] == 2018]\n",
        "test_df = df[df['Year'] == 2019]\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train_raw = scaler.fit_transform(train_df[feature_cols].values)\n",
        "X_val_raw = scaler.transform(val_df[feature_cols].values)\n",
        "X_test_raw = scaler.transform(test_df[feature_cols].values)\n",
        "\n",
        "y_train = train_df[target_col].values\n",
        "y_val = val_df[target_col].values\n",
        "y_test = test_df[target_col].values\n",
        "\n",
        "def create_sequences(data, target, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        xs.append(data[i:(i + seq_length)])\n",
        "        ys.append(target[i + seq_length])\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "X_train, y_train = create_sequences(X_train_raw, y_train, CONFIG['SEQ_LENGTH'])\n",
        "X_val, y_val = create_sequences(X_val_raw, y_val, CONFIG['SEQ_LENGTH'])\n",
        "X_test, y_test = create_sequences(X_test_raw, y_test, CONFIG['SEQ_LENGTH'])\n",
        "\n",
        "print(f\" >> Data Split | Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "SqCskzW2-MK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 2. Model Factory (Including Transformer)\n",
        "# ---------------------------------------------------------\n",
        "def get_benchmark_model(model_name, input_shape, conf):\n",
        "    \"\"\"\n",
        "    Constructs the specified model architecture using fixed hyperparameters.\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Unpack Config\n",
        "    u = conf['UNITS']\n",
        "    dr = conf['DROPOUT']\n",
        "    act = conf['ACTIVATION']\n",
        "    ks = conf['KERNEL_SIZE']\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    # --- Architectures ---\n",
        "    if model_name == 'RNN':\n",
        "        x = SimpleRNN(u, activation=act, return_sequences=False)(x)\n",
        "        x = Dropout(dr)(x)\n",
        "\n",
        "    elif model_name == 'LSTM':\n",
        "        x = LSTM(u, activation=act, return_sequences=False)(x)\n",
        "        x = Dropout(dr)(x)\n",
        "\n",
        "    elif model_name == 'GRU':\n",
        "        x = GRU(u, activation=act, return_sequences=False)(x)\n",
        "        x = Dropout(dr)(x)\n",
        "\n",
        "    elif model_name == 'BiLSTM':\n",
        "        x = Bidirectional(LSTM(u, activation=act, return_sequences=False))(x)\n",
        "        x = Dropout(dr)(x)\n",
        "\n",
        "    elif model_name == 'CNN_1D':\n",
        "        x = Conv1D(filters=u, kernel_size=ks, activation=act, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling1D(pool_size=2)(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Dropout(dr)(x)\n",
        "\n",
        "    elif model_name == 'CNN_BiLSTM':\n",
        "        x = Conv1D(filters=u, kernel_size=ks, activation=act, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling1D(pool_size=2)(x)\n",
        "        x = Dropout(dr)(x)\n",
        "        x = Bidirectional(LSTM(u, activation=act, return_sequences=False))(x)\n",
        "        x = Dropout(dr)(x)\n",
        "\n",
        "    elif model_name == 'Attn_GRU':\n",
        "        gru_out = GRU(u, activation=act, return_sequences=True)(x)\n",
        "        # Self-Attention\n",
        "        attn_out = MultiHeadAttention(num_heads=conf['ATTN_HEADS'], key_dim=u)(gru_out, gru_out)\n",
        "        x = Add()([gru_out, attn_out])\n",
        "        x = LayerNormalization()(x)\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "        x = Dropout(dr)(x)\n",
        "\n",
        "    elif model_name == 'TCN':\n",
        "        # Simplified TCN Block with Residuals\n",
        "        for dilation_rate in [1, 2, 4]:\n",
        "            prev_x = x\n",
        "            conv = Conv1D(filters=u, kernel_size=ks, dilation_rate=dilation_rate,\n",
        "                          padding='causal', activation=act)(x)\n",
        "            conv = BatchNormalization()(conv)\n",
        "            conv = Dropout(0.1)(conv) # Light internal dropout\n",
        "\n",
        "            # Match dimensions if needed\n",
        "            if prev_x.shape[-1] != u:\n",
        "                prev_x = Conv1D(filters=u, kernel_size=1, padding='same', activation=act)(prev_x)\n",
        "\n",
        "            x = Add()([prev_x, conv])\n",
        "\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "        x = Dropout(dr)(x)\n",
        "\n",
        "    elif model_name == 'Transformer':\n",
        "        # Standard Transformer Encoder\n",
        "        attn_output = MultiHeadAttention(num_heads=conf['ATTN_HEADS'], key_dim=u)(x, x)\n",
        "        x = Add()([x, attn_output])\n",
        "        x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Feed Forward Network\n",
        "        ffn = Dense(conf['FF_DIM'], activation=act)(x)\n",
        "        ffn = Dense(input_shape[-1])(ffn)\n",
        "        x = Add()([x, ffn])\n",
        "        x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        x = GlobalAveragePooling1D()(x) # Pooling instead of LSTM\n",
        "        x = Dropout(dr)(x)\n",
        "\n",
        "    # --- Output Head ---\n",
        "    x = Dense(32, activation=act)(x)\n",
        "    outputs = Dense(1, activation='relu')(x) # Non-negative power\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs, name=model_name)\n",
        "    model.compile(optimizer=Adam(learning_rate=conf['LEARNING_RATE']),\n",
        "                  loss=conf['LOSS'], metrics=['mae'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "xkGPPfsk-OQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 3. Benchmarking Loop\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n--- [Step 2] Conducting Benchmark (9 Models) ---\")\n",
        "\n",
        "model_list = [\n",
        "    'RNN', 'LSTM', 'GRU', 'BiLSTM',\n",
        "    'CNN_1D', 'CNN_BiLSTM', 'Attn_GRU', 'TCN', 'Transformer'\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "# Prepare Combined Data for Retraining\n",
        "X_combined = np.concatenate((X_train, X_val), axis=0)\n",
        "y_combined = np.concatenate((y_train, y_val), axis=0)\n",
        "\n",
        "for name in model_list:\n",
        "    print(f\"\\n>> Evaluating Model: {name}\")\n",
        "\n",
        "    # 1. Validation Phase (Find Best Epoch)\n",
        "    model = get_benchmark_model(name, (CONFIG['SEQ_LENGTH'], len(feature_cols)), CONFIG)\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    hist = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=CONFIG['EPOCHS'],\n",
        "        batch_size=CONFIG['BATCH_SIZE'],\n",
        "        callbacks=[early_stop],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    best_epoch = np.argmin(hist.history['val_loss']) + 1\n",
        "    val_loss = min(hist.history['val_loss'])\n",
        "    print(f\"   -> Optimal Epoch: {best_epoch} (Val MSE: {val_loss:.4f})\")\n",
        "\n",
        "    # 2. Retraining Phase (Combined Data)\n",
        "    print(f\"   -> Retraining on full dataset...\")\n",
        "    final_model = get_benchmark_model(name, (CONFIG['SEQ_LENGTH'], len(feature_cols)), CONFIG)\n",
        "    final_model.fit(\n",
        "        X_combined, y_combined,\n",
        "        epochs=best_epoch,\n",
        "        batch_size=CONFIG['BATCH_SIZE'],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # 3. Testing Phase\n",
        "    y_pred = final_model.predict(X_test, verbose=0)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"   -> [Result] RMSE: {rmse:.4f} | MAE: {mae:.4f} | R2: {r2:.4f}\")\n",
        "\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2\n",
        "    })"
      ],
      "metadata": {
        "id": "0A_PdAo--QoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 4. Final Reporting\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n--- [Step 3] Final Benchmark Report ---\")\n",
        "\n",
        "# Table\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n[Table: Deep Learning Models Comparison]\")\n",
        "print(results_df)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = plt.bar(results_df['Model'], results_df['RMSE'], color='dimgray', alpha=0.8)\n",
        "plt.title('Benchmark Comparison: Deep Learning Models (RMSE)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('RMSE (kW) - Lower is Better', fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Highlight lowest RMSE\n",
        "min_rmse_idx = results_df['RMSE'].idxmin()\n",
        "bars[min_rmse_idx].set_color('crimson')\n",
        "bars[min_rmse_idx].set_alpha(1.0)\n",
        "\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 1.0, f\"{yval:.2f}\",\n",
        "             ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n[Analysis]\")\n",
        "print(f\"1. The model with the lowest RMSE is '{results_df.loc[min_rmse_idx, 'Model']}'.\")\n",
        "print(\"2. 'Transformer' results indicate the performance of pure self-attention without local convolution.\")\n",
        "print(\"3. Comparing 'CNN_BiLSTM' and 'Attn_GRU' reveals the trade-off between local feature extraction and global context.\")"
      ],
      "metadata": {
        "id": "Aoz2-JvG-SKb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}