{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5OAuA8A8TtL"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Project: Trend-Context Fusion Network (TCFN) for Solar Power Forecasting\n",
        "# File: main_tcfn_pipeline.ipynb\n",
        "# Description: This script implements the full TCFN pipeline, including:\n",
        "#              1. Data Preprocessing & Cyclical Encoding\n",
        "#              2. Grid Search for Hyperparameter Optimization\n",
        "#              3. Retraining Strategy on Combined Dataset\n",
        "#              4. Final Evaluation (RMSE, MAE, R2)\n",
        "#              5. XAI Analysis using SHAP (Feature Importance & Local Explanation)\n",
        "# Environment: Google Colab / TensorFlow 2.x\n",
        "# =============================================================================\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 0. Setup & Configuration\n",
        "# ---------------------------------------------------------\n",
        "!pip install shap -q\n",
        "\n",
        "import os\n",
        "import random\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Dense, Dropout, LayerNormalization,\n",
        "                                     MultiHeadAttention, Add, Concatenate,\n",
        "                                     LSTM, Conv1D, BatchNormalization)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Robust Reproducibility Setup\n",
        "SEED = 42\n",
        "def set_seeds(seed=SEED):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    try:\n",
        "        tf.config.experimental.enable_op_determinism()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "set_seeds()\n",
        "SEQ_LENGTH = 24"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 1. Data Loading & Feature Engineering\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n--- [Step 1] Data Acquisition & Preprocessing ---\")\n",
        "print(\"Please upload the dataset (e.g., 'Dangjin_Landfill_PV_Dataset.csv').\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))\n",
        "print(f\" >> File '{filename}' uploaded successfully.\")\n",
        "\n",
        "df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "\n",
        "def apply_feature_engineering(df):\n",
        "    \"\"\"\n",
        "    Applies cyclical time encoding to capture seasonal and diurnal patterns.\n",
        "    \"\"\"\n",
        "    df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "    df['DaysInYear'] = df['Date'].dt.is_leap_year.apply(lambda x: 366 if x else 365)\n",
        "\n",
        "    # Yearly Seasonality\n",
        "    df['Day_sin'] = np.sin(2 * np.pi * df['DayOfYear'] / df['DaysInYear'])\n",
        "    df['Day_cos'] = np.cos(2 * np.pi * df['DayOfYear'] / df['DaysInYear'])\n",
        "\n",
        "    # Daily Seasonality\n",
        "    df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24.0)\n",
        "    df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24.0)\n",
        "    return df\n",
        "\n",
        "df = apply_feature_engineering(df)\n",
        "\n",
        "# Define Features and Target\n",
        "weather_cols = ['AverageTemp', 'LowTemp', 'HighTemp', 'RainFall', 'SteamPress',\n",
        "                'DewPoint', 'Sunshine', 'Insolation', 'Cloudiness', 'GroundTemp',\n",
        "                'Temp', 'Wind', 'Press', 'Humi']\n",
        "time_cols = ['Day_sin', 'Day_cos', 'Hour_sin', 'Hour_cos']\n",
        "feature_cols = weather_cols + time_cols\n",
        "target_col = 'Solar_Power'\n",
        "\n",
        "# Chronological Split\n",
        "train_df = df[df['Year'].isin([2015, 2016, 2017])]\n",
        "val_df = df[df['Year'] == 2018]\n",
        "test_df = df[df['Year'] == 2019]\n",
        "\n",
        "print(f\" >> Data Split | Train: {train_df.shape}, Val: {val_df.shape}, Test: {test_df.shape}\")\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train_raw = scaler.fit_transform(train_df[feature_cols].values)\n",
        "X_val_raw = scaler.transform(val_df[feature_cols].values)\n",
        "X_test_raw = scaler.transform(test_df[feature_cols].values)\n",
        "\n",
        "y_train_raw = train_df[target_col].values\n",
        "y_val_raw = val_df[target_col].values\n",
        "y_test_raw = test_df[target_col].values\n",
        "\n",
        "def create_sequences(data, target, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        xs.append(data[i:(i + seq_length)])\n",
        "        ys.append(target[i + seq_length])\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "X_train, y_train = create_sequences(X_train_raw, y_train_raw, SEQ_LENGTH)\n",
        "X_val, y_val = create_sequences(X_val_raw, y_val_raw, SEQ_LENGTH)\n",
        "X_test, y_test = create_sequences(X_test_raw, y_test_raw, SEQ_LENGTH)"
      ],
      "metadata": {
        "id": "jZV3NRY78ZbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 2. Model Architecture: TCFN\n",
        "# ---------------------------------------------------------\n",
        "def trend_aware_attention_block(x, num_heads=4, key_dim=16):\n",
        "    \"\"\"Adds a residual Multi-Head Attention block with LayerNorm.\"\"\"\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
        "    x = Add()([x, attn_output])\n",
        "    x = LayerNormalization()(x)\n",
        "\n",
        "    ffn = Dense(x.shape[-1], activation='swish')(x)\n",
        "    x = Add()([x, ffn])\n",
        "    x = LayerNormalization()(x)\n",
        "    return x\n",
        "\n",
        "def build_tcfn_model(input_shape, params):\n",
        "    \"\"\"Builds the Trend-Context Fusion Network based on hyperparameters.\"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # 1. Local Trend Extraction (Conv1D)\n",
        "    trend = Conv1D(params['lstm_units'], kernel_size=3, padding='same', activation='swish')(inputs)\n",
        "    trend = BatchNormalization()(trend)\n",
        "    trend = Dropout(params['dropout'])(trend)\n",
        "\n",
        "    # 2. Global Context Mining (Attention)\n",
        "    context = trend_aware_attention_block(inputs)\n",
        "\n",
        "    # 3. Fusion & Sequential Modeling (LSTM)\n",
        "    combined = Concatenate()([trend, context])\n",
        "    x = LSTM(params['lstm_units'])(combined)\n",
        "    x = Dropout(params['dropout'])(x)\n",
        "\n",
        "    # 4. Regression Head\n",
        "    x = Dense(params['lstm_units'], activation='swish')(x)\n",
        "    outputs = Dense(1, activation='relu')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(params['lr']), loss='mse', metrics=['mae'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "j_h2dcaJ8beZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 3. Hyperparameter Grid Search\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n--- [Step 2] Hyperparameter Grid Search (Experimental Design) ---\")\n",
        "\n",
        "scenarios = [\n",
        "    {'id': 'Light_Fast',        'lstm_units': 32,  'lr': 0.001,  'dropout': 0.2, 'desc': 'Fast Convergence'},\n",
        "    {'id': 'Standard_Balanced', 'lstm_units': 64,  'lr': 0.0005, 'dropout': 0.3, 'desc': 'Baseline Capacity'},\n",
        "    {'id': 'Deep_Slow',         'lstm_units': 128, 'lr': 0.0001, 'dropout': 0.3, 'desc': 'Fine-tuning'},\n",
        "    {'id': 'Deep_Robust',       'lstm_units': 128, 'lr': 0.0005, 'dropout': 0.5, 'desc': 'High Regularization'}\n",
        "]\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_params = {}\n",
        "best_epoch_num = 0\n",
        "\n",
        "for sc in scenarios:\n",
        "    print(f\"\\n>> Experiment: {sc['id']} ({sc['desc']})\")\n",
        "\n",
        "    model = build_tcfn_model((SEQ_LENGTH, len(feature_cols)), sc)\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    hist = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=30,\n",
        "        batch_size=64,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    min_loss = min(hist.history['val_loss'])\n",
        "    opt_epoch = np.argmin(hist.history['val_loss']) + 1\n",
        "\n",
        "    print(f\"   -> Val MSE: {min_loss:.4f} (at Epoch {opt_epoch})\")\n",
        "\n",
        "    if min_loss < best_val_loss:\n",
        "        best_val_loss = min_loss\n",
        "        best_params = sc\n",
        "        best_epoch_num = opt_epoch\n",
        "\n",
        "print(f\"\\n[Selected Configuration] {best_params['id']}\")\n",
        "print(f\" >> Optimal Retraining Epochs: {best_epoch_num}\")"
      ],
      "metadata": {
        "id": "lo6dNMo18eh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 4. Retraining Strategy (Train + Val)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n--- [Step 3] Retraining on Combined Dataset ---\")\n",
        "\n",
        "X_combined = np.concatenate((X_train, X_val), axis=0)\n",
        "y_combined = np.concatenate((y_train, y_val), axis=0)\n",
        "\n",
        "final_model = build_tcfn_model((SEQ_LENGTH, len(feature_cols)), best_params)\n",
        "\n",
        "final_model.fit(\n",
        "    X_combined, y_combined,\n",
        "    epochs=best_epoch_num,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "GLJDLjI68iXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 5. Final Evaluation\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n--- [Step 4] Final Evaluation on Test Set (2019) ---\")\n",
        "\n",
        "y_pred = final_model.predict(X_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\" >> RMSE: {rmse:.4f}\")\n",
        "print(f\" >> MAE : {mae:.4f}\")\n",
        "print(f\" >> R2  : {r2:.4f}\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(y_test[:200], label='Actual Power', color='navy', alpha=0.7)\n",
        "plt.plot(y_pred[:200], label='TCFN Prediction', color='crimson', alpha=0.7, linestyle='--')\n",
        "plt.title(f'Forecast Comparison (First 200 Hours) | R2: {r2:.3f}', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Solar Power (kW)')\n",
        "plt.xlabel('Time (Hours)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W-vgvty29j8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 6. Explainable AI (SHAP)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n--- [Step 5] XAI Analysis: SHAP ---\")\n",
        "print(\"Computing SHAP values (this may take time)...\")\n",
        "\n",
        "# Wrapper for SHAP (3D -> 2D -> 3D)\n",
        "def model_predict_wrapper(data_flattened):\n",
        "    data_reshaped = data_flattened.reshape(-1, SEQ_LENGTH, len(feature_cols))\n",
        "    return final_model.predict(data_reshaped, verbose=0).flatten()\n",
        "\n",
        "# Background & Test Samples\n",
        "background_data = X_train[np.random.choice(X_train.shape[0], 50, replace=False)]\n",
        "background_flat = background_data.reshape(50, -1)\n",
        "\n",
        "test_idx = np.random.choice(X_test.shape[0], 10, replace=False)\n",
        "test_samples = X_test[test_idx]\n",
        "test_flat = test_samples.reshape(10, -1)\n",
        "\n",
        "explainer = shap.KernelExplainer(model_predict_wrapper, background_flat)\n",
        "shap_values = explainer.shap_values(test_flat, nsamples=100)\n",
        "\n",
        "# --- Visualization 1: Global Feature Importance ---\n",
        "print(\"\\n[Visualizing Global Feature Importance]\")\n",
        "shap_matrix = np.array(shap_values)\n",
        "shap_3d = shap_matrix.reshape(-1, SEQ_LENGTH, len(feature_cols))\n",
        "feature_imp = np.abs(shap_3d).mean(axis=(0, 1))\n",
        "\n",
        "indices = np.argsort(feature_imp)\n",
        "sorted_feats = [feature_cols[i] for i in indices]\n",
        "sorted_vals = feature_imp[indices]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(len(feature_cols)), sorted_vals, color='steelblue', align='center')\n",
        "plt.yticks(range(len(feature_cols)), sorted_feats)\n",
        "plt.xlabel('Mean |SHAP Value| (Impact on Output)')\n",
        "plt.title('Global Feature Importance', fontsize=12, fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "# --- Visualization 2: Local Explanation (Waterfall) ---\n",
        "print(\"\\n[Visualizing Local Explanation (Sample #0)]\")\n",
        "sample_id = 0\n",
        "feat_names_flat = [f\"{col}_t{t}\" for t in range(SEQ_LENGTH) for col in feature_cols]\n",
        "\n",
        "explanation = shap.Explanation(\n",
        "    values=shap_values[sample_id],\n",
        "    base_values=explainer.expected_value,\n",
        "    data=test_flat[sample_id],\n",
        "    feature_names=feat_names_flat\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "shap.plots.waterfall(explanation, max_display=15, show=False)\n",
        "plt.title(f'Local Explanation for Sample Index {test_idx[sample_id]}', fontsize=12, fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAll analysis completed successfully.\")"
      ],
      "metadata": {
        "id": "7HVCnPmy9lkZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
