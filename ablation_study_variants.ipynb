{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5OAuA8A8TtL"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Project: Trend-Context Fusion Network (TCFN) for PV Power Forecasting\n",
        "# File: ablation_study_variants.ipynb\n",
        "# Description: This script performs an ablation study to analyze the impact of\n",
        "#              removing specific modules (Conv1D, Attention, LSTM, Cyclical Encoding)\n",
        "#              from the architecture.\n",
        "# Environment: Google Colab / TensorFlow 2.x\n",
        "# =============================================================================\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 0. Setup & Configuration\n",
        "# ---------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from google.colab import files\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Dense, Dropout, LayerNormalization,\n",
        "                                     MultiHeadAttention, Add, Concatenate, LSTM,\n",
        "                                     Conv1D, BatchNormalization, GlobalAveragePooling1D)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Configuration for Reproducibility\n",
        "SEED = 42\n",
        "def set_seeds(seed=SEED):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "set_seeds()\n",
        "\n",
        "# Hyperparameters & Constants\n",
        "SEQ_LENGTH = 24\n",
        "H_PARAMS = {\n",
        "    'lstm_units': 128,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.0005,\n",
        "    'n_heads': 4   # For Attention Mechanism\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 1. Data Loading & Preprocessing\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n--- [Step 1] Data Upload ---\")\n",
        "print(\"Please upload your dataset file.\")\n",
        "print(\"Recommended files: 'Dangjin_Landfill_PV_Dataset.csv' or 'Gwangyang_Port_Phase2_PV.csv'\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))\n",
        "print(f\" >> File '{filename}' uploaded successfully.\")\n",
        "\n",
        "df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "\n",
        "def add_cyclical_features(df):\n",
        "    \"\"\"\n",
        "    Adds cyclical time encoding (Sine/Cosine) for Day of Year and Hour of Day.\n",
        "    This helps the model capture seasonal and diurnal patterns explicitly.\n",
        "    \"\"\"\n",
        "    df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "    df['DaysInYear'] = df['Date'].dt.is_leap_year.apply(lambda x: 366 if x else 365)\n",
        "\n",
        "    # Yearly Seasonality\n",
        "    df['Day_sin'] = np.sin(2 * np.pi * df['DayOfYear'] / df['DaysInYear'])\n",
        "    df['Day_cos'] = np.cos(2 * np.pi * df['DayOfYear'] / df['DaysInYear'])\n",
        "\n",
        "    # Daily Seasonality\n",
        "    df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24.0)\n",
        "    df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24.0)\n",
        "    return df\n",
        "\n",
        "# Apply Feature Engineering\n",
        "df = add_cyclical_features(df)\n",
        "\n",
        "# Feature Definition\n",
        "weather_cols = ['AverageTemp', 'LowTemp', 'HighTemp', 'RainFall', 'SteamPress',\n",
        "                'DewPoint', 'Sunshine', 'Insolation', 'Cloudiness', 'GroundTemp',\n",
        "                'Temp', 'Wind', 'Press', 'Humi']\n",
        "time_cols = ['Day_sin', 'Day_cos', 'Hour_sin', 'Hour_cos']\n",
        "full_features = weather_cols + time_cols\n",
        "target_col = 'Solar_Power'\n",
        "\n",
        "# Data Partitioning (Chronological Split)\n",
        "# Train: 2015-2017 | Val: 2018 | Test: 2019\n",
        "train_df = df[df['Year'].isin([2015, 2016, 2017])]\n",
        "val_df = df[df['Year'] == 2018]\n",
        "test_df = df[df['Year'] == 2019]\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train_raw = scaler.fit_transform(train_df[full_features].values)\n",
        "X_val_raw = scaler.transform(val_df[full_features].values)\n",
        "X_test_raw = scaler.transform(test_df[full_features].values)\n",
        "\n",
        "y_train = train_df[target_col].values\n",
        "y_val = val_df[target_col].values\n",
        "y_test = test_df[target_col].values\n",
        "\n",
        "def create_sequences(data, target, seq_length):\n",
        "    \"\"\"Generates time-series sequences for LSTM input.\"\"\"\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        xs.append(data[i:(i + seq_length)])\n",
        "        ys.append(target[i + seq_length])\n",
        "    return np.array(xs), np.array(ys)"
      ],
      "metadata": {
        "id": "jZV3NRY78ZbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 2. Model Architecture (Flexible Builder)\n",
        "# ---------------------------------------------------------\n",
        "def build_ablation_model(input_shape, model_type='TCFN'):\n",
        "    \"\"\"\n",
        "    Constructs the model architecture based on the specified variant.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Shape of the input data (Sequence Length, Features).\n",
        "        model_type: Variant name ('TCFN', 'No_Conv', 'No_Attn', 'No_LSTM').\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # --- Module 1: Local Trend Extraction (Conv1D) ---\n",
        "    if model_type != 'No_Conv':\n",
        "        trend = Conv1D(H_PARAMS['lstm_units'], kernel_size=3, padding='same', activation='swish')(inputs)\n",
        "        trend = BatchNormalization()(trend)\n",
        "        trend = Dropout(H_PARAMS['dropout'])(trend)\n",
        "    else:\n",
        "        trend = None\n",
        "\n",
        "    # --- Module 2: Global Context Mining (Multi-Head Attention) ---\n",
        "    if model_type != 'No_Attn':\n",
        "        context = MultiHeadAttention(num_heads=H_PARAMS['n_heads'], key_dim=16)(inputs, inputs)\n",
        "        context = Add()([inputs, context]) # Residual Connection\n",
        "        context = LayerNormalization()(context)\n",
        "        ffn_c = Dense(inputs.shape[-1], activation='swish')(context)\n",
        "        context = Add()([context, ffn_c])\n",
        "        context = LayerNormalization()(context)\n",
        "    else:\n",
        "        context = inputs\n",
        "\n",
        "    # --- Feature Fusion Strategy ---\n",
        "    if model_type == 'No_Conv':\n",
        "        combined = context\n",
        "    elif model_type == 'No_Attn':\n",
        "        combined = trend\n",
        "    else:\n",
        "        # Concatenate both pathways (for 'No_LSTM' or 'No_Cyclic' variants)\n",
        "        combined = Concatenate()([trend, context])\n",
        "\n",
        "    # --- Module 3: Sequential Modeling (LSTM) ---\n",
        "    if model_type != 'No_LSTM':\n",
        "        x = LSTM(H_PARAMS['lstm_units'])(combined)\n",
        "    else:\n",
        "        # Ablation: Replace LSTM with Global Average Pooling\n",
        "        x = GlobalAveragePooling1D()(combined)\n",
        "        x = Dense(H_PARAMS['lstm_units'], activation='swish')(x)\n",
        "\n",
        "    x = Dropout(H_PARAMS['dropout'])(x)\n",
        "\n",
        "    # --- Regression Head ---\n",
        "    x = Dense(H_PARAMS['lstm_units'], activation='swish')(x)\n",
        "    outputs = Dense(1, activation='relu')(x) # ReLU for non-negative power output\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=H_PARAMS['lr']), loss='mse', metrics=['mae'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "j_h2dcaJ8beZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 3. Experiment Execution (Ablation Variants Only)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n--- [Step 2] Conducting Ablation Study (Variants Only) ---\")\n",
        "\n",
        "# Define Ablation Scenarios (Excluding the Full Proposed Model)\n",
        "experiments = [\n",
        "    {'name': 'w/o Conv1D',        'type': 'No_Conv', 'use_cyclic': True},\n",
        "    {'name': 'w/o Attention',     'type': 'No_Attn', 'use_cyclic': True},\n",
        "    {'name': 'w/o LSTM',          'type': 'No_LSTM', 'use_cyclic': True},\n",
        "    {'name': 'w/o Cyclic Enc.',   'type': 'TCFN',    'use_cyclic': False}\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for exp in experiments:\n",
        "    print(f\"\\n>> Running Experiment: {exp['name']} ...\")\n",
        "\n",
        "    # Select Features (Handle 'w/o Cyclic Enc.' case)\n",
        "    if exp['use_cyclic']:\n",
        "        features_idx = list(range(len(full_features)))\n",
        "    else:\n",
        "        # Exclude the last 4 cyclical time features\n",
        "        features_idx = list(range(len(weather_cols)))\n",
        "\n",
        "    # Prepare Data for this specific experiment\n",
        "    X_train_exp, y_train_exp = create_sequences(X_train_raw[:, features_idx], y_train, SEQ_LENGTH)\n",
        "    X_val_exp, y_val_exp = create_sequences(X_val_raw[:, features_idx], y_val, SEQ_LENGTH)\n",
        "    X_test_exp, y_test_exp = create_sequences(X_test_raw[:, features_idx], y_test, SEQ_LENGTH)\n",
        "\n",
        "    # Build Model\n",
        "    model = build_ablation_model(input_shape=(SEQ_LENGTH, len(features_idx)), model_type=exp['type'])\n",
        "\n",
        "    # Training with Early Stopping\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    model.fit(\n",
        "        X_train_exp, y_train_exp,\n",
        "        validation_data=(X_val_exp, y_val_exp),\n",
        "        epochs=30,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=0 # Suppress epoch logs for cleaner output\n",
        "    )\n",
        "\n",
        "    # Evaluation\n",
        "    y_pred = model.predict(X_test_exp, verbose=0)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_exp, y_pred))\n",
        "    mae = mean_absolute_error(y_test_exp, y_pred)\n",
        "    r2 = r2_score(y_test_exp, y_pred)\n",
        "\n",
        "    print(f\"   -> [Result] RMSE: {rmse:.4f} | MAE: {mae:.4f} | R2: {r2:.4f}\")\n",
        "\n",
        "    results.append({\n",
        "        'Model Variant': exp['name'],\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2\n",
        "    })"
      ],
      "metadata": {
        "id": "lo6dNMo18eh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 4. Final Reporting & Visualization\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n--- [Step 3] Final Experimental Report ---\")\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n[Table: Ablation Variants Results]\")\n",
        "print(results_df)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Colors: All gray since we are comparing variants\n",
        "bars = plt.bar(results_df['Model Variant'], results_df['RMSE'], color='gray', alpha=0.85)\n",
        "\n",
        "plt.title('Ablation Study: Performance of Reduced Variants (RMSE)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('RMSE (kW) - Lower is Better', fontsize=12)\n",
        "plt.xlabel('Ablation Variants', fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 1.0, f\"{yval:.2f}\",\n",
        "             ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "\n",
        "plt.xticks(rotation=30)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n[Summary Analysis]\")\n",
        "print(\"1. All variants represent partial architectures of the TCFN framework.\")\n",
        "print(\"2. The RMSE values indicate the error magnitude introduced by removing each specific component.\")\n",
        "print(\"3. High error rates in specific variants (e.g., 'w/o LSTM') highlight critical dependencies for the dataset.\")"
      ],
      "metadata": {
        "id": "GLJDLjI68iXk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}